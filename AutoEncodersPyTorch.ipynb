{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"u2U7TIRdKNGu"},"source":["# [Autoencoders](https://arxiv.org/abs/2201.03898)\n","\n","## PS6 - Put your answers in these Markdown cells by editing them. You'll find the questions in various Markdown cells"]},{"cell_type":"markdown","metadata":{"id":"it5nmu_-c5-E"},"source":["When training CNNs, one of the problems is that we need a lot of labeled data. In the case of image classification, we need to separate images into different classes, which is a manual effort.\n","\n","However, we might want to use raw (unlabeled) data for training CNN feature extractors, which is called **self-supervised learning**. Instead of labels, we will use training images as both network input and output. The main idea of **autoencoder** is that we will have an **encoder network** that converts input image into some **latent space** (normally it is just a vector of some smaller size), then the **decoder network**, whose goal would be to reconstruct the original image.\n","\n","Since we are training autoencoder to capture as much of the information from the original image as possible for accurate reconstruction, the network tries to find the best **embedding** of input images to capture the meaning.\n","\n","![AutoEncoder Diagram](images/autoencoder_schema.jpg)\n","\n","> Image from [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)\n","\n","Let's create simplest autoencoder for MNIST!"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.262403Z","iopub.status.busy":"2022-04-08T00:15:39.262149Z","iopub.status.idle":"2022-04-08T00:15:39.268143Z","shell.execute_reply":"2022-04-08T00:15:39.267475Z","shell.execute_reply.started":"2022-04-08T00:15:39.262376Z"},"id":"6n8fzzN-2T1h","trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from torch import nn\n","from torch import optim\n","from tqdm import tqdm\n","import numpy as np\n","import torch.nn.functional as F\n","torch.manual_seed(42)\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["Define training parameters and check if the GPU is available:"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.530041Z","iopub.status.busy":"2022-04-08T00:15:39.529522Z","iopub.status.idle":"2022-04-08T00:15:39.534247Z","shell.execute_reply":"2022-04-08T00:15:39.533327Z","shell.execute_reply.started":"2022-04-08T00:15:39.530006Z"},"id":"bjL-jOgi3gmG","trusted":true},"outputs":[],"source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","train_size = 0.9\n","lr = 1e-3\n","eps = 1e-8\n","batch_size = 256\n","epochs = 30"]},{"cell_type":"markdown","metadata":{},"source":["The following function will load the MNIST dataset and apply specified transforms to it. It will also split it into train/test datasets."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.893727Z","iopub.status.busy":"2022-04-08T00:15:39.893160Z","iopub.status.idle":"2022-04-08T00:15:39.899407Z","shell.execute_reply":"2022-04-08T00:15:39.898565Z","shell.execute_reply.started":"2022-04-08T00:15:39.893688Z"},"id":"g2Eo43713Sxb","trusted":true},"outputs":[],"source":["def mnist(train_part, transform=None):\n","    dataset = torchvision.datasets.MNIST('.', download=True, transform=transform)\n","    train_part = int(train_part * len(dataset))\n","    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_part, len(dataset) - train_part])\n","    return train_dataset, test_dataset"]},{"cell_type":"markdown","metadata":{},"source":["Now let's load the dataset and define dataloaders for train and test:"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:41.292476Z","iopub.status.busy":"2022-04-08T00:15:41.292238Z","iopub.status.idle":"2022-04-08T00:15:41.299801Z","shell.execute_reply":"2022-04-08T00:15:41.298897Z","shell.execute_reply.started":"2022-04-08T00:15:41.292449Z"},"id":"jAI3uK86_zHM","trusted":true},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor()])\n","\n","train_dataset, test_dataset = mnist(train_size, transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, drop_last=True, batch_size=batch_size, shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n","dataloaders = (train_dataloader, test_dataloader)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:41.692950Z","iopub.status.busy":"2022-04-08T00:15:41.692376Z","iopub.status.idle":"2022-04-08T00:15:41.700707Z","shell.execute_reply":"2022-04-08T00:15:41.699723Z","shell.execute_reply.started":"2022-04-08T00:15:41.692909Z"},"id":"LdyQz4092fRV","trusted":true},"outputs":[],"source":["\n","def plotn(n, data, noisy=False, super_res=None):\n","    fig, ax = plt.subplots(1, n)\n","    for i, z in enumerate(data):\n","        if i == n:\n","            break\n","        preprocess = z[0].reshape(1, 28, 28) if z[0].shape[1] == 28 else z[0].reshape(1, 14, 14) if z[0].shape[1] == 14 else z[0]\n","        if super_res is not None:\n","            _transform = transforms.Resize((int(preprocess.shape[1] / super_res), int(preprocess.shape[2] / super_res)))\n","            preprocess = _transform(preprocess)\n","\n","        if noisy:\n","            shapes = list(preprocess.shape)\n","            preprocess += noisify(shapes)\n","\n","        ax[i].imshow(preprocess[0])\n","    plt.show()"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:10:46.485196Z","iopub.status.busy":"2022-04-08T01:10:46.484414Z","iopub.status.idle":"2022-04-08T01:10:46.489587Z","shell.execute_reply":"2022-04-08T01:10:46.488769Z","shell.execute_reply.started":"2022-04-08T01:10:46.485158Z"},"id":"FjpCEs-oWu6_","trusted":true},"outputs":[],"source":["def noisify(shapes):\n","    return np.random.normal(loc=0.5, scale=0.3, size=shapes)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:42.470833Z","iopub.status.busy":"2022-04-08T00:15:42.470589Z","iopub.status.idle":"2022-04-08T00:15:42.855919Z","shell.execute_reply":"2022-04-08T00:15:42.853605Z","shell.execute_reply.started":"2022-04-08T00:15:42.470808Z"},"id":"NeWJoiFC4A6J","outputId":"e680eb07-bf94-4301-8742-852103885624","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plotn(5, train_dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Use the Pytorch documentation and videos https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html to answer these questions\n","\n","### What are the different \"types\" of layers being used in the Encoder below and say in words what they do?\n","    Convolutional layers - scan image for spatial correlations and detects features of an image using a filter scanning over the entire image\n","    Pooling layers - reduce the dimension\n","    ReLU layer - non-linear activation function\n","### What are the arguments to Conv2d \n","    input channels: number of channels in the input image\n","    output features: number of desired output features after convolution\n","    kernel size: the size of the filter being applies\n","    padding: the number of rows/columns of zeros to add to \"pad\" the input image so that each actual point in the image is used in the same number of convolutions.\n","### What does maxpool do?  \n","    Takes only the maximum value of the kernal, which reduces the size of the image.\n","### What does ReLU do? \n","    An activation function to introduce non-linearity into the model. If the input is positive, the function returns the input, otherwise is returns a zero."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:42.857823Z","iopub.status.busy":"2022-04-08T00:15:42.857536Z","iopub.status.idle":"2022-04-08T00:15:42.866008Z","shell.execute_reply":"2022-04-08T00:15:42.865357Z","shell.execute_reply.started":"2022-04-08T00:15:42.857787Z"},"id":"NnI2YvOg4DbT","trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding='same')\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding='same')\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv3 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), padding=(1, 1))\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        hidden1 = self.maxpool1(self.relu(self.conv1(input)))\n","        hidden2 = self.maxpool2(self.relu(self.conv2(hidden1)))\n","        encoded = self.maxpool3(self.relu(self.conv3(hidden2)))\n","        return encoded"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### What does upsample do? \n","    Upsample is a type of layer used to increase the dimension of an input. It interpolates missing values and increases the dimensions of the image."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:43.169659Z","iopub.status.busy":"2022-04-08T00:15:43.169239Z","iopub.status.idle":"2022-04-08T00:15:43.178700Z","shell.execute_reply":"2022-04-08T00:15:43.178010Z","shell.execute_reply.started":"2022-04-08T00:15:43.169622Z"},"id":"mZGB4Vr47478","trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        self.upsample1 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv2 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        self.upsample2 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv3 = nn.Conv2d(8, 16, kernel_size=(3, 3))\n","        self.upsample3 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv4 = nn.Conv2d(16, 1, kernel_size=(3, 3), padding='same')\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input):\n","        hidden1 = self.upsample1(self.relu(self.conv1(input)))\n","        hidden2 = self.upsample2(self.relu(self.conv2(hidden1)))\n","        hidden3 = self.upsample3(self.relu(self.conv3(hidden2)))\n","        decoded = self.sigmoid(self.conv4(hidden3))\n","        return decoded"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:43.593770Z","iopub.status.busy":"2022-04-08T00:15:43.593178Z","iopub.status.idle":"2022-04-08T00:15:43.600152Z","shell.execute_reply":"2022-04-08T00:15:43.599205Z","shell.execute_reply.started":"2022-04-08T00:15:43.593731Z"},"id":"SDGiAPhbBBLY","trusted":true},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","    def __init__(self, super_resolution=False):\n","        super().__init__()\n","        if not super_resolution:\n","            self.encoder = Encoder()\n","        else:\n","            self.encoder = SuperResolutionEncoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, input):\n","        encoded = self.encoder(input)\n","        decoded = self.decoder(encoded)\n","        return decoded"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:44.069329Z","iopub.status.busy":"2022-04-08T00:15:44.068875Z","iopub.status.idle":"2022-04-08T00:15:44.079486Z","shell.execute_reply":"2022-04-08T00:15:44.078748Z","shell.execute_reply.started":"2022-04-08T00:15:44.069289Z"},"id":"nZyG_mNu_Pnc","trusted":true},"outputs":[],"source":["model = AutoEncoder().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","loss_fn = nn.BCELoss()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### What does the optimizer do in the code below? \n","    The optimizer updates the parameters of the model in order to minimize the loss function on the training data.\n","### Which optimizer is being used?\n","    Adam - Adaptive Moment Estimatation\n","    Uses a combination of stochastic gradient descent and momentum-based optimization algorithms. It is less computationally demanding than other methods."]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:25:58.828126Z","iopub.status.busy":"2022-04-08T01:25:58.827634Z","iopub.status.idle":"2022-04-08T01:25:58.844248Z","shell.execute_reply":"2022-04-08T01:25:58.843270Z","shell.execute_reply.started":"2022-04-08T01:25:58.828092Z"},"id":"iiIy87v2_rUr","trusted":true},"outputs":[],"source":["def train(dataloaders, model, loss_fn, optimizer, epochs, device, noisy=None, super_res=None):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","\n","    for epoch in tqdm_iter:\n","        model.train()\n","        train_loss = 0.0\n","        test_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            shapes = list(imgs.shape)\n","\n","            if super_res is not None:\n","                shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n","                _transform = transforms.Resize((shapes[2], shapes[3]))\n","                imgs_transformed = _transform(imgs)\n","                imgs_transformed = imgs_transformed.to(device)\n","\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            if noisy is not None:\n","                noisy_tensor = noisy[0]\n","            else:\n","                noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n","\n","            if super_res is None:\n","                imgs_noisy = imgs + noisy_tensor\n","            else:\n","                imgs_noisy = imgs_transformed + noisy_tensor\n","\n","            imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n","\n","            preds = model(imgs_noisy)\n","            loss = loss_fn(preds, imgs)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                shapes = list(imgs.shape)\n","\n","                if super_res is not None:\n","                    shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n","                    _transform = transforms.Resize((shapes[2], shapes[3]))\n","                    imgs_transformed = _transform(imgs)\n","                    imgs_transformed = imgs_transformed.to(device)\n","\n","\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","\n","                if noisy is not None:\n","                    test_noisy_tensor = noisy[1]\n","                else:\n","                    test_noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n","\n","                if super_res is None:\n","                    imgs_noisy = imgs + test_noisy_tensor\n","                else:\n","                    imgs_noisy = imgs_transformed + test_noisy_tensor\n","\n","                imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n","\n","                preds = model(imgs_noisy)\n","                loss = loss_fn(preds, imgs)\n","\n","                test_loss += loss.item()\n","\n","        train_loss /= len(train_dataloader)\n","        test_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:44.969616Z","iopub.status.busy":"2022-04-08T00:15:44.969345Z","iopub.status.idle":"2022-04-08T00:22:34.533469Z","shell.execute_reply":"2022-04-08T00:22:34.532667Z","shell.execute_reply.started":"2022-04-08T00:15:44.969587Z"},"id":"PMqO8eOxCemz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [20:12<00:00, 40.42s/it, train loss:=0.104, test loss:=0.104]\n"]}],"source":["train(dataloaders, model, loss_fn, optimizer, epochs, device)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:22:34.535491Z","iopub.status.busy":"2022-04-08T00:22:34.535154Z","iopub.status.idle":"2022-04-08T00:22:35.701465Z","shell.execute_reply":"2022-04-08T00:22:35.700797Z","shell.execute_reply.started":"2022-04-08T00:22:34.535452Z"},"id":"kR3n0EnjOts0","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["model.eval()\n","predictions = []\n","plots = 5\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    predictions.append(model(data[0].to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, test_dataset)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{"id":"1JUmf9i1dg2i"},"source":["> **Task 1**: Try to train autoencoder with very small latent vector size, eg. 2, and plot the dots corresponding to different digits. *Hint: Use fully-connected dense layer after the convoluitonal part to reduce the vector size to the required value.*\n","\n","> **Task 2**: Starting from different digits, obtain their latent space representations, and see what effect adding some noise to the latent space has on the resulting digits."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 3","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     80\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m train(model, train_loader, optimizer, criterion, device, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m \u001b[39m# Encode the test dataset and plot the results\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n","Cell \u001b[0;32mIn[36], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(autoencoder, train_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     48\u001b[0m outputs \u001b[39m=\u001b[39m autoencoder(images)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, images)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/loss.py:530\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/functional.py:3279\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3276\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3277\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3279\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3280\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 3"]}],"source":["import matplotlib.pyplot as plt\n","\n","# Define the autoencoder architecture\n","class Autoencoder(nn.Module):\n","    def __init__(self, latent_dim=2):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64 * 4 * 4, latent_dim),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, 64 * 4 * 4),\n","            nn.ReLU(),\n","            nn.Unflatten(-1, (64, 4, 4)),\n","            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","\n","# Define the training function\n","def train(autoencoder, train_loader, optimizer, criterion, device, num_epochs):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for images, _ in train_loader:\n","            # Move data to device\n","            images = images.to(device)\n","\n","            # Forward pass\n","            outputs = autoencoder(images)\n","            loss = criterion(outputs, images)\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        # Print progress\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n","\n","\n","# Load the MNIST dataset\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n","\n","# Set up the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the model and optimizer\n","model = Autoencoder(latent_dim=2).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","# Train the model\n","train(model, train_loader, optimizer, criterion, device, num_epochs=10)\n","\n","# Encode the test dataset and plot the results\n","with torch.no_grad():\n","    encoded_imgs = []\n","    labels = []\n","    for images, target in train_loader:\n","        images = images.to(device)\n","        encoded = model.encoder(images)\n","        encoded_imgs.append(encoded.cpu().detach())\n","        labels.append(target)\n","    encoded_imgs = torch.cat(encoded_imgs, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get encoder part of model\n","encoder = model.encoder\n","\n","# Pass digit images through encoder to get latent space representations\n","digit1 = test_dataset[0][0].to(device).unsqueeze(0)\n","latent1 = encoder(digit1)\n","\n","digit2 = test_dataset[1][0].to(device).unsqueeze(0)\n","latent2 = encoder(digit2)\n","\n","digit3 = test_dataset[2][0].to(device).unsqueeze(0)\n","latent3 = encoder(digit3)\n","\n","# Add noise to the latent space representations\n","noisy_latent1 = latent1 + torch.randn_like(latent1) * 0.2\n","noisy_latent2 = latent2 + torch.randn_like(latent2) * 0.2\n","noisy_latent3 = latent3 + torch.randn_like(latent3) * 0.2\n","\n","# Generate images from the noisy latent space representations\n","decoder = model.decoder\n","recon1 = decoder(noisy_latent1)\n","recon2 = decoder(noisy_latent2)\n","recon3 = decoder(noisy_latent3)\n","\n","# Plot the original digits and their reconstructions\n","plotn(3, [digit1, recon1.cpu(), digit2, recon2.cpu(), digit3, recon3.cpu()])\n"]},{"cell_type":"markdown","metadata":{"id":"tjdFt03rULX-"},"source":["## Denoising\n","\n","Autoencoders can be effectively used to remove noise from images. In order to train denoiser, we will start with noise-free images, and add artificial noise to them. Then, we will feed autoencoder with noisy images as input, and noise-free images as output.\n","\n","Let's see how this works for MNIST:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:26:06.259333Z","iopub.status.busy":"2022-04-08T01:26:06.259061Z","iopub.status.idle":"2022-04-08T01:26:06.799264Z","shell.execute_reply":"2022-04-08T01:26:06.798609Z","shell.execute_reply.started":"2022-04-08T01:26:06.259303Z"},"id":"1Yj9ZRDmUPxX","outputId":"7e3a1624-4326-41a2-8f94-3d41e11cd411","trusted":true},"outputs":[],"source":["plotn(5, train_dataset, noisy=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:45.901599Z","iopub.status.busy":"2022-04-08T01:55:45.901279Z","iopub.status.idle":"2022-04-08T01:55:45.910993Z","shell.execute_reply":"2022-04-08T01:55:45.909858Z","shell.execute_reply.started":"2022-04-08T01:55:45.901533Z"},"id":"qxo8NDLLUvut","trusted":true},"outputs":[],"source":["model = AutoEncoder().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:47.302746Z","iopub.status.busy":"2022-04-08T01:55:47.302202Z","iopub.status.idle":"2022-04-08T01:55:47.316135Z","shell.execute_reply":"2022-04-08T01:55:47.315439Z","shell.execute_reply.started":"2022-04-08T01:55:47.302710Z"},"trusted":true},"outputs":[],"source":["noisy_tensor = torch.FloatTensor(noisify([256, 1, 28, 28])).to(device)\n","test_noisy_tensor = torch.FloatTensor(noisify([1, 1, 28, 28])).to(device)\n","noisy_tensors = (noisy_tensor, test_noisy_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:47.956516Z","iopub.status.busy":"2022-04-08T01:55:47.956250Z","iopub.status.idle":"2022-04-08T02:18:17.428958Z","shell.execute_reply":"2022-04-08T02:18:17.428239Z","shell.execute_reply.started":"2022-04-08T01:55:47.956489Z"},"id":"JAYzgfoTUBHM","trusted":true},"outputs":[],"source":["train(dataloaders, model, loss_fn, optimizer, 100, device, noisy=noisy_tensors)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T02:18:17.430982Z","iopub.status.busy":"2022-04-08T02:18:17.430570Z","iopub.status.idle":"2022-04-08T02:18:18.235054Z","shell.execute_reply":"2022-04-08T02:18:18.234364Z","shell.execute_reply.started":"2022-04-08T02:18:17.430943Z"},"id":"IaPfyJ0SV7XY","trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","noise = []\n","plots = 5\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    shapes = data[0].shape\n","    noisy_data = data[0] + test_noisy_tensor[0].detach().cpu()\n","    noise.append(noisy_data)\n","    predictions.append(model(noisy_data.to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, noise)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{"id":"MyvmZEHzdoJT"},"source":["> **Exercise:** See how denoiser trained on MNIST digits works for different images. As an example, you can take [Fashion MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST) dataset, which has the same image size. Note that denoiser works well only on the same image type that it was trained on (i.e. for the same probability distribution of input data)."]},{"cell_type":"markdown","metadata":{"id":"yxFKm_OxdqfO"},"source":["## Super-Resolution\n","\n","Similarly to denoiser, we can train autoencoders to increase the resolution of the image. To train super-resolution network, we will start with high-resolution images, and automatically downscale them to produce network inputs. We will then feed autoencoder with small images as inputs and high-resolution images as outputs.\n","\n","For that let's downscale image to 14x14 at train."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.310634Z","iopub.status.busy":"2022-04-08T00:31:45.310384Z","iopub.status.idle":"2022-04-08T00:31:45.688313Z","shell.execute_reply":"2022-04-08T00:31:45.687614Z","shell.execute_reply.started":"2022-04-08T00:31:45.310597Z"},"id":"trida5guu9js","outputId":"3dd13ba4-0351-4982-c162-578735af23f8","trusted":true},"outputs":[],"source":["super_res_koeff = 2.0\n","plotn(5, train_dataset, super_res=super_res_koeff)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.690022Z","iopub.status.busy":"2022-04-08T00:31:45.689620Z","iopub.status.idle":"2022-04-08T00:31:45.698351Z","shell.execute_reply":"2022-04-08T00:31:45.697586Z","shell.execute_reply.started":"2022-04-08T00:31:45.689984Z"},"id":"9vC57e-rei4p","trusted":true},"outputs":[],"source":["class SuperResolutionEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding='same')\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding='same')\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), padding=(1, 1))\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        hidden1 = self.maxpool1(self.relu(self.conv1(input)))\n","        encoded = self.maxpool2(self.relu(self.conv2(hidden1)))\n","        return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.699896Z","iopub.status.busy":"2022-04-08T00:31:45.699576Z","iopub.status.idle":"2022-04-08T00:31:45.714684Z","shell.execute_reply":"2022-04-08T00:31:45.713859Z","shell.execute_reply.started":"2022-04-08T00:31:45.699857Z"},"id":"d78J288qe5qJ","trusted":true},"outputs":[],"source":["model = AutoEncoder(super_resolution=True).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.718432Z","iopub.status.busy":"2022-04-08T00:31:45.718047Z","iopub.status.idle":"2022-04-08T00:38:29.683824Z","shell.execute_reply":"2022-04-08T00:38:29.683115Z","shell.execute_reply.started":"2022-04-08T00:31:45.718402Z"},"id":"CJ_zcN5Je6I-","outputId":"2311bd2a-55c6-4fdf-c4d2-37f0426fb61c","trusted":true},"outputs":[],"source":["train(dataloaders, model, loss_fn, optimizer, epochs, device, super_res=2.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:29.687844Z","iopub.status.busy":"2022-04-08T00:38:29.687353Z","iopub.status.idle":"2022-04-08T00:38:30.585026Z","shell.execute_reply":"2022-04-08T00:38:30.584352Z","shell.execute_reply.started":"2022-04-08T00:38:29.687803Z"},"id":"YsVfcCKKfjv1","outputId":"e7e029b0-b403-4feb-f869-d6367957741a","trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","plots = 5\n","shapes = test_dataset[0][0].shape\n","\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    _transform = transforms.Resize((int(shapes[1] / super_res_koeff), int(shapes[2] / super_res_koeff)))\n","    predictions.append(model(_transform(data[0]).to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, test_dataset, super_res=super_res_koeff)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{"id":"-aZqFJthfu9-"},"source":["> **Exercise**: Try to train super-resolution network on [CIFAR-10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html) for 2x and 4x upscaling. Use noise as input to 4x upscaling model and observe the result."]},{"cell_type":"markdown","metadata":{"id":"A3DOJU1-gTJV"},"source":["# [Variational Auto-Encoders (VAE)](https://arxiv.org/abs/1906.02691)\n","\n","Traditional autoencoders reduce the dimension of the input data somehow, figuring out the important features of input images. However, latent vectors often do not make much sense. In other words, taking MNIST dataset as an example, figuring out which digits correspond to different latent vectors is not an easy task, because close latent vectors would not necessarily correspond to the same digits. \n","\n","On the other hand, to train *generative* models it is better to have some understanding of the latent space. This idea leads us to **variational auto-encoder** (VAE).\n","\n","VAE is the autoencoder that learns to predict *statistical distribution* of the latent parameters, so-called **latent distribution**. For example, we can assume that latent vectors would be distributed as $N(\\mathrm{z\\_mean},e^{\\mathrm{z\\_log}})$, where $\\mathrm{z\\_mean}, \\mathrm{z\\_log} \\in\\mathbb{R}^d$. Encoder in VAE learns to predict those parameters, and then decoder takes a random vector from this distribution to reconstruct the object.\n","\n","To summarize:\n","\n"," * From input vector, we predict `z_mean` and `z_log` (instead of predicting the standard deviation itself, we predict it's logarithm)\n"," * We sample a vector `sample(z_val in code)` from the distribution $N(\\mathrm{z\\_mean},e^{\\mathrm{z\\_log\\_sigma}})$\n"," * Decoder tries to decode the original image using `sample` as an input vector\n","\n"," <img src=\"images/vae.png\" width=\"50%\">\n","\n"," > Image from [this blog post](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) by Isaak Dykeman"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.586882Z","iopub.status.busy":"2022-04-08T00:38:30.586394Z","iopub.status.idle":"2022-04-08T00:38:30.595011Z","shell.execute_reply":"2022-04-08T00:38:30.594391Z","shell.execute_reply.started":"2022-04-08T00:38:30.586844Z"},"id":"aT_GEWeU409I","trusted":true},"outputs":[],"source":["class VAEEncoder(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.intermediate_dim = 512\n","        self.latent_dim = 2\n","        self.linear = nn.Linear(784, self.intermediate_dim)\n","        self.z_mean = nn.Linear(self.intermediate_dim, self.latent_dim)\n","        self.z_log = nn.Linear(self.intermediate_dim, self.latent_dim)\n","        self.relu = nn.ReLU()\n","        self.device = device\n","\n","    def forward(self, input):\n","        bs = input.shape[0]\n","\n","        hidden = self.relu(self.linear(input))\n","        z_mean = self.z_mean(hidden)\n","        z_log = self.z_log(hidden)\n","\n","        eps = torch.FloatTensor(np.random.normal(size=(bs, self.latent_dim))).to(device)\n","        z_val = z_mean + torch.exp(z_log) * eps\n","        return z_mean, z_log, z_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.596177Z","iopub.status.busy":"2022-04-08T00:38:30.595932Z","iopub.status.idle":"2022-04-08T00:38:30.606805Z","shell.execute_reply":"2022-04-08T00:38:30.606054Z","shell.execute_reply.started":"2022-04-08T00:38:30.596144Z"},"id":"XWi4oCcq409p","trusted":true},"outputs":[],"source":["class VAEDecoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.intermediate_dim = 512\n","        self.latent_dim = 2\n","        self.linear = nn.Linear(self.latent_dim, self.intermediate_dim)\n","        self.output = nn.Linear(self.intermediate_dim, 784)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input):\n","        hidden = self.relu(self.linear(input))\n","        decoded = self.sigmoid(self.output(hidden))\n","        return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.608150Z","iopub.status.busy":"2022-04-08T00:38:30.607836Z","iopub.status.idle":"2022-04-08T00:38:30.618464Z","shell.execute_reply":"2022-04-08T00:38:30.617740Z","shell.execute_reply.started":"2022-04-08T00:38:30.608114Z"},"id":"WukDYQ9f409p","trusted":true},"outputs":[],"source":["class VAEAutoEncoder(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.encoder = VAEEncoder(device)\n","        self.decoder = VAEDecoder()\n","        self.z_vals = None\n","\n","    def forward(self, input):\n","        bs, c, h, w = input.shape[0], input.shape[1], input.shape[2], input.shape[3]\n","        input = input.view(bs, -1)\n","        encoded = self.encoder(input)\n","        self.z_vals = encoded\n","        decoded = self.decoder(encoded[2])\n","        return decoded\n","    \n","    def get_zvals(self):\n","        return self.z_vals"]},{"cell_type":"markdown","metadata":{},"source":["Variational auto-encoders use complex loss function that consists of two parts:\n","* **Reconstruction loss** is the loss function that shows how close reconstructed image is to the target (can be MSE). It is the same loss function as in normal autoencoders.\n","* **KL loss**, which ensures that latent variable distributions stays close to normal distribution. It is based on the notion of [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - a metric to estimate how similar two statistical distributions are."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.620121Z","iopub.status.busy":"2022-04-08T00:38:30.619868Z","iopub.status.idle":"2022-04-08T00:38:30.627984Z","shell.execute_reply":"2022-04-08T00:38:30.627265Z","shell.execute_reply.started":"2022-04-08T00:38:30.620089Z"},"trusted":true},"outputs":[],"source":["def vae_loss(preds, targets, z_vals):\n","    mse = nn.MSELoss()\n","    reconstruction_loss = mse(preds, targets.view(targets.shape[0], -1)) * 784.0\n","    temp = 1.0 + z_vals[1] - torch.square(z_vals[0]) - torch.exp(z_vals[1])\n","    kl_loss = -0.5 * torch.sum(temp, axis=-1)\n","    return torch.mean(reconstruction_loss + kl_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.629709Z","iopub.status.busy":"2022-04-08T00:38:30.629192Z","iopub.status.idle":"2022-04-08T00:38:30.648407Z","shell.execute_reply":"2022-04-08T00:38:30.647801Z","shell.execute_reply.started":"2022-04-08T00:38:30.629671Z"},"trusted":true},"outputs":[],"source":["model = VAEAutoEncoder(device).to(device)\n","optimizer = optim.RMSprop(model.parameters(), lr=lr, eps=eps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.649871Z","iopub.status.busy":"2022-04-08T00:38:30.649560Z","iopub.status.idle":"2022-04-08T00:38:30.659437Z","shell.execute_reply":"2022-04-08T00:38:30.658769Z","shell.execute_reply.started":"2022-04-08T00:38:30.649835Z"},"trusted":true},"outputs":[],"source":["def train_vae(dataloaders, model, optimizer, epochs, device):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","\n","    for epoch in tqdm_iter:\n","        model.train()\n","        train_loss = 0.0\n","        test_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(imgs)\n","            z_vals = model.get_zvals()\n","            loss = vae_loss(preds, imgs, z_vals)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","\n","                preds = model(imgs)\n","                z_vals = model.get_zvals()\n","                loss = vae_loss(preds, imgs, z_vals)\n","\n","                test_loss += loss.item()\n","\n","        train_loss /= len(train_dataloader)\n","        test_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.661256Z","iopub.status.busy":"2022-04-08T00:38:30.660812Z","iopub.status.idle":"2022-04-08T00:43:25.674280Z","shell.execute_reply":"2022-04-08T00:43:25.673608Z","shell.execute_reply.started":"2022-04-08T00:38:30.661216Z"},"trusted":true},"outputs":[],"source":["train_vae(dataloaders, model, optimizer, epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:25.676045Z","iopub.status.busy":"2022-04-08T00:43:25.675634Z","iopub.status.idle":"2022-04-08T00:43:26.429255Z","shell.execute_reply":"2022-04-08T00:43:26.428437Z","shell.execute_reply.started":"2022-04-08T00:43:25.676007Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","plots = 5\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    predictions.append(model(data[0].to(device).unsqueeze(0)).view(1, 28, 28).detach().cpu())\n","plotn(plots, test_dataset)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{},"source":["> **Task**: In our sample, we have trained fully-connected VAE. Now take the CNN from traditional auto-encoder above and create CNN-based VAE."]},{"cell_type":"markdown","metadata":{},"source":["# [Adversarial Auto-Encoders (AAE)](https://arxiv.org/abs/1511.05644)"]},{"cell_type":"markdown","metadata":{},"source":["Adversarial Auto-Encoders is a **combination** of Generative Adversarial Networks and Variational Auto-Encoders. \n","\n","Encoder will be the generator, discriminator will learn to distinguish the real images encoder output from generated ones. Encoder output is a distribution, from this output decoder will try decode image.\n","\n","In this approach we have **three loss functions**: generator loss, discriminator loss from GAN's and reconstruction loss from VAE.\n","\n"," <img src=\"images/aae.png\" width=\"50%\">\n","\n"," > Image from [this blog post](https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/) by Felipe Ducau"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.430881Z","iopub.status.busy":"2022-04-08T00:43:26.430435Z","iopub.status.idle":"2022-04-08T00:43:26.437703Z","shell.execute_reply":"2022-04-08T00:43:26.437030Z","shell.execute_reply.started":"2022-04-08T00:43:26.430837Z"},"trusted":true},"outputs":[],"source":["class AAEEncoder(nn.Module):\n","    def __init__(self, input_dim, inter_dim, latent_dim):\n","        super().__init__()\n","        self.linear1 = nn.Linear(input_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, latent_dim)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        encoded = self.linear4(hidden3)\n","        return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.439383Z","iopub.status.busy":"2022-04-08T00:43:26.438973Z","iopub.status.idle":"2022-04-08T00:43:26.450914Z","shell.execute_reply":"2022-04-08T00:43:26.450120Z","shell.execute_reply.started":"2022-04-08T00:43:26.439339Z"},"trusted":true},"outputs":[],"source":["class AAEDecoder(nn.Module):\n","    def __init__(self, latent_dim, inter_dim, output_dim):\n","        super().__init__()\n","        self.linear1 = nn.Linear(latent_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, output_dim)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        decoded = self.sigmoid(self.linear4(hidden3))\n","        return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.453838Z","iopub.status.busy":"2022-04-08T00:43:26.453577Z","iopub.status.idle":"2022-04-08T00:43:26.462584Z","shell.execute_reply":"2022-04-08T00:43:26.461884Z","shell.execute_reply.started":"2022-04-08T00:43:26.453814Z"},"trusted":true},"outputs":[],"source":["class AAEDiscriminator(nn.Module):\n","    def __init__(self, latent_dim, inter_dim):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.inter_dim = inter_dim\n","        self.linear1 = nn.Linear(latent_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, inter_dim)\n","        self.linear5 = nn.Linear(inter_dim, 1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        hidden4 = self.relu(self.linear4(hidden3))\n","        decoded = self.sigmoid(self.linear4(hidden4))\n","        return decoded\n","    \n","    def get_dims(self):\n","        return self.latent_dim, self.inter_dim\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.464666Z","iopub.status.busy":"2022-04-08T00:43:26.464215Z","iopub.status.idle":"2022-04-08T00:43:26.474821Z","shell.execute_reply":"2022-04-08T00:43:26.474117Z","shell.execute_reply.started":"2022-04-08T00:43:26.464628Z"},"trusted":true},"outputs":[],"source":["input_dims = 784\n","inter_dims = 1000\n","latent_dims = 150"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.477879Z","iopub.status.busy":"2022-04-08T00:43:26.477199Z","iopub.status.idle":"2022-04-08T00:43:26.537720Z","shell.execute_reply":"2022-04-08T00:43:26.537052Z","shell.execute_reply.started":"2022-04-08T00:43:26.477797Z"},"trusted":true},"outputs":[],"source":["aae_encoder = AAEEncoder(input_dims, inter_dims, latent_dims).to(device)\n","aae_decoder = AAEDecoder(latent_dims, inter_dims, input_dims).to(device)\n","aae_discriminator = AAEDiscriminator(latent_dims, int(inter_dims / 2)).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.539592Z","iopub.status.busy":"2022-04-08T00:43:26.539109Z","iopub.status.idle":"2022-04-08T00:43:26.543949Z","shell.execute_reply":"2022-04-08T00:43:26.543156Z","shell.execute_reply.started":"2022-04-08T00:43:26.539555Z"},"trusted":true},"outputs":[],"source":["lr = 1e-4\n","regularization_lr = 5e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.545858Z","iopub.status.busy":"2022-04-08T00:43:26.545404Z","iopub.status.idle":"2022-04-08T00:43:26.553377Z","shell.execute_reply":"2022-04-08T00:43:26.552700Z","shell.execute_reply.started":"2022-04-08T00:43:26.545825Z"},"trusted":true},"outputs":[],"source":["optim_encoder = optim.Adam(aae_encoder.parameters(), lr=lr)\n","optim_encoder_regularization = optim.Adam(aae_encoder.parameters(), lr=regularization_lr)\n","optim_decoder = optim.Adam(aae_decoder.parameters(), lr=lr)\n","optim_discriminator = optim.Adam(aae_discriminator.parameters(), lr=regularization_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.555945Z","iopub.status.busy":"2022-04-08T00:43:26.555659Z","iopub.status.idle":"2022-04-08T00:43:26.576630Z","shell.execute_reply":"2022-04-08T00:43:26.575929Z","shell.execute_reply.started":"2022-04-08T00:43:26.555908Z"},"trusted":true},"outputs":[],"source":["def train_aae(dataloaders, models, optimizers, epochs, device):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","    \n","    enc, dec, disc = models[0], models[1], models[2]\n","    optim_enc, optim_enc_reg, optim_dec, optim_disc = optimizers[0], optimizers[1], optimizers[2], optimizers[3]\n","    \n","    eps = 1e-9\n","\n","    for epoch in tqdm_iter:\n","        enc.train()\n","        dec.train()\n","        disc.train()\n","\n","        train_reconst_loss = 0.0\n","        train_disc_loss = 0.0\n","        train_enc_loss = 0.0\n","        \n","        test_reconst_loss = 0.0\n","        test_disc_loss = 0.0\n","        test_enc_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            imgs = imgs.view(imgs.shape[0], -1).to(device)\n","            labels = labels.to(device)\n","            \n","            enc.zero_grad()\n","            dec.zero_grad()\n","            disc.zero_grad()\n","             \n","            encoded = enc(imgs)\n","            decoded = dec(encoded)\n","            \n","            reconstruction_loss = F.binary_cross_entropy(decoded, imgs)\n","            reconstruction_loss.backward()\n","            \n","            optim_enc.step()\n","            optim_dec.step()\n","            enc.eval()\n","\n","            latent_dim, disc_inter_dim = disc.get_dims()\n","            real = torch.randn(imgs.shape[0], latent_dim).to(device)\n","            \n","            disc_real = disc(real)\n","            disc_fake = disc(enc(imgs))\n","            \n","            disc_loss = -torch.mean(torch.log(disc_real + eps) + torch.log(1.0 - disc_fake + eps))\n","            disc_loss.backward()\n","            \n","            optim_dec.step()\n","            enc.train()\n","            \n","            disc_fake = disc(enc(imgs))\n","            enc_loss = -torch.mean(torch.log(disc_fake + eps))\n","            enc_loss.backward()\n","            \n","            optim_enc_reg.step()\n","\n","            train_reconst_loss += reconstruction_loss.item()\n","            train_disc_loss += disc_loss.item()\n","            train_enc_loss += enc_loss.item()\n","\n","        enc.eval()\n","        dec.eval()\n","        disc.eval()\n","\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                imgs = imgs.view(imgs.shape[0], -1).to(device)\n","                labels = labels.to(device)\n","\n","                encoded = enc(imgs)\n","                decoded = dec(encoded)\n","\n","                reconstruction_loss = F.binary_cross_entropy(decoded, imgs)\n","\n","                latent_dim, disc_inter_dim = disc.get_dims()\n","                real = torch.randn(imgs.shape[0], latent_dim).to(device)\n","\n","                disc_real = disc(real)\n","                disc_fake = disc(enc(imgs))\n","                disc_loss = -torch.mean(torch.log(disc_real + eps) + torch.log(1.0 - disc_fake + eps))\n","\n","                disc_fake = disc(enc(imgs))\n","                enc_loss = -torch.mean(torch.log(disc_fake + eps))\n","\n","                test_reconst_loss += reconstruction_loss.item()\n","                test_disc_loss += disc_loss.item()\n","                test_enc_loss += enc_loss.item()\n","\n","        train_reconst_loss /= len(train_dataloader)\n","        train_disc_loss /= len(train_dataloader)\n","        train_enc_loss /= len(train_dataloader)\n","        \n","        test_reconst_loss /= len(test_dataloader)\n","        test_disc_loss /= len(test_dataloader)\n","        test_enc_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train reconst loss:': train_reconst_loss, 'train disc loss:': train_disc_loss, 'train enc loss': train_enc_loss, \\\n","                        'test reconst loss:': test_reconst_loss, 'test disc loss:': test_disc_loss, 'test enc loss': test_enc_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.578419Z","iopub.status.busy":"2022-04-08T00:43:26.577933Z","iopub.status.idle":"2022-04-08T00:43:26.589197Z","shell.execute_reply":"2022-04-08T00:43:26.588532Z","shell.execute_reply.started":"2022-04-08T00:43:26.578373Z"},"trusted":true},"outputs":[],"source":["models = (aae_encoder, aae_decoder, aae_discriminator)\n","optimizers = (optim_encoder, optim_encoder_regularization, optim_decoder, optim_discriminator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.592138Z","iopub.status.busy":"2022-04-08T00:43:26.590197Z","iopub.status.idle":"2022-04-08T00:52:49.002321Z","shell.execute_reply":"2022-04-08T00:52:49.001592Z","shell.execute_reply.started":"2022-04-08T00:43:26.592099Z"},"trusted":true},"outputs":[],"source":["train_aae(dataloaders, models, optimizers, epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:52:49.004318Z","iopub.status.busy":"2022-04-08T00:52:49.003842Z","iopub.status.idle":"2022-04-08T00:52:50.737055Z","shell.execute_reply":"2022-04-08T00:52:50.736379Z","shell.execute_reply.started":"2022-04-08T00:52:49.004280Z"},"trusted":true},"outputs":[],"source":["aae_encoder.eval()\n","aae_decoder.eval()\n","predictions = []\n","plots = 10\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    pred = aae_decoder(aae_encoder(data[0].to(device).unsqueeze(0).view(1, 784)))\n","    predictions.append(pred.view(1, 28, 28).detach().cpu())\n","plotn(plots, test_dataset)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Additional Materials\n","\n","* [Blog post on NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)\n","* [Variational Autoencoders Explained](https://kvfrans.com/variational-autoencoders-explained/)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
